<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://stedylan.github.io</id>
    <title>Gridea</title>
    <updated>2021-05-25T08:26:20.195Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://stedylan.github.io"/>
    <link rel="self" href="https://stedylan.github.io/atom.xml"/>
    <subtitle>æ¸©æ•…è€ŒçŸ¥æ–°</subtitle>
    <logo>https://stedylan.github.io/images/avatar.png</logo>
    <icon>https://stedylan.github.io/favicon.ico</icon>
    <rights>All rights reserved 2021, Gridea</rights>
    <entry>
        <title type="html"><![CDATA[pytorch]]></title>
        <id>https://stedylan.github.io/post/pytorchåˆå­¦å›¾åƒåˆ†ç±»å™¨/</id>
        <link href="https://stedylan.github.io/post/pytorchåˆå­¦å›¾åƒåˆ†ç±»å™¨/">
        </link>
        <updated>2021-12-11T16:00:00.000Z</updated>
        <content type="html"><![CDATA[<h1 id="è®­ç»ƒä¸€ä¸ªå›¾åƒåˆ†ç±»å™¨">è®­ç»ƒä¸€ä¸ªå›¾åƒåˆ†ç±»å™¨</h1>
<p>1.ä½¿ç”¨torchvisionåŠ è½½å’Œæ­£åˆ™åŒ–CIFAR10è®­ç»ƒé›†å’Œæµ‹è¯•é›†</p>
<p>2.æ„é€ ä¸€ä¸ªCNN</p>
<p>3.æ„é€ æŸå¤±å‡½æ•°</p>
<p>4.è®­ç»ƒç½‘ç»œ</p>
<p>5.æµ‹è¯•ç½‘ç»œ</p>
<h2 id="1-åŠ è½½å’Œæ­£è§„åŒ–cifar10">1. åŠ è½½å’Œæ­£è§„åŒ–CIFAR10</h2>
<pre><code class="language-python">import torch
import torchvision
import torchvision.transforms as transforms
</code></pre>
<pre><code class="language-python">transform = transforms.Compose(
                                [transforms.ToTensor(),
                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])

trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)

testset = torchvision.datasets.CIFAR10(root='./data', train=False,
                                       download=True, transform=transform)
testloader = torch.utils.data.DataLoader(testset, batch_size=4,
                                         shuffle=False, num_workers=2)

classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')
</code></pre>
<pre><code>Files already downloaded and verified
Files already downloaded and verified
</code></pre>
<pre><code class="language-python">import matplotlib.pyplot as plt
import numpy as np

def imshow(img):
    img = img / 2 + 0.5
    npimg = img.numpy()
    plt.imshow(np.transpose(npimg, (1, 2, 0)))
    plt.show()

dataiter = iter(trainloader)
images, labels = dataiter.next()

imshow(torchvision.utils.make_grid(images))
print(' '.join('%5s' % classes[labels[j]] for j in range(4)))
</code></pre>
<figure data-type="image" tabindex="1"><img src="https://img-blog.csdnimg.cn/20201222101039601.png#pic_center" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°" loading="lazy"></figure>
<pre><code>truck   car  bird  ship
</code></pre>
<h2 id="2-å®šä¹‰ä¸€ä¸ªå·ç§¯ç¥ç»ç½‘ç»œ">2. å®šä¹‰ä¸€ä¸ªå·ç§¯ç¥ç»ç½‘ç»œ</h2>
<pre><code class="language-python">import torch.nn as nn
import torch.nn.functional as F

device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(3, 6, 5)
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(6, 16, 5)
        self.fc1 = nn.Linear(16 * 5 * 5, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = x.view(-1, 16 * 5 * 5)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x


net = Net()
net.to(device)
</code></pre>
<pre><code>Net(
  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=400, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=84, bias=True)
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
</code></pre>
<h2 id="3-å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨">3. å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨</h2>
<pre><code class="language-python">import torch.optim as optim

criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9) # momentumä¸ºåŠ¨é‡ï¼Œè§£å†³å±€éƒ¨æœ€ä¼˜è§£é—®é¢˜
</code></pre>
<h2 id="4-è®­ç»ƒcnn">4. è®­ç»ƒCNN</h2>
<pre><code class="language-python">
for epoch in range(2):
    running_loss = 0.0
    for i, data in enumerate(trainloader, 0):
        # inputs, labels = data # ä½¿ç”¨cpuè®­ç»ƒ
        inputs, labels = data[0].to(device), data[1].to(device) # ä½¿ç”¨gpuè®­ç»ƒ

        optimizer.zero_grad()

        outputs = net(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        if i % 2000 == 1999: # æ¯ä¸¤åƒæ¬¡ï¼Œè®¡ç®—ä¸€ä¸‹lossçš„æ€»å’Œï¼Œçœ‹æ€»ä½“çš„é™ä½
            print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 2000))
            running_loss = 0.0

print('Finished Training')
</code></pre>
<pre><code>[1,  2000] loss: 2.139
[1,  4000] loss: 1.818
[1,  6000] loss: 1.678
[1,  8000] loss: 1.575
[1, 10000] loss: 1.523
[1, 12000] loss: 1.490
[2,  2000] loss: 1.417
[2,  4000] loss: 1.345
[2,  6000] loss: 1.356
[2,  8000] loss: 1.314
[2, 10000] loss: 1.291
[2, 12000] loss: 1.279
Finished Training
</code></pre>
<pre><code class="language-python">PATH = './cifar_net.pth'
torch.save(net.state_dict(), PATH)
</code></pre>
<h2 id="5-æµ‹è¯•ç½‘ç»œ">5. æµ‹è¯•ç½‘ç»œ</h2>
<pre><code class="language-python">dataiter = iter(testloader) # dataloadè¿­ä»£å™¨
images, labels = dataiter.next() 

# print images
imshow(torchvision.utils.make_grid(images))
print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4)))
</code></pre>
<figure data-type="image" tabindex="2"><img src="https://img-blog.csdnimg.cn/20201222101054704.png#pic_center" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°" loading="lazy"></figure>
<pre><code>GroundTruth:    cat  ship  ship plane
</code></pre>
<pre><code class="language-python">net = Net()
net.load_state_dict(torch.load(PATH))
outputs = net(images)
</code></pre>
<pre><code class="language-python">_, predicted = torch.max(outputs, 1) # dim = 1, å°†ç»´åº¦å‹ç¼©ä¸º1
print(outputs) # æ¯ä¸€è¡Œä»£è¡¨ä¸€ä¸ªå›¾ç‰‡çš„è¾“å‡ºå€¼
print(predicted) # æ¯å¼ å›¾ç‰‡æœ€å¤§è¾“å‡ºå€¼çš„ä¸‹è¾¹

print('Predicted: ', ' '.join('%5s' % classes[predicted[j]]
                              for j in range(4)))
</code></pre>
<pre><code>tensor([[-2.5090, -3.2349,  1.7301,  2.9694,  3.0115,  2.8149,  3.4539,  0.7578,
         -3.6328, -3.4352],
        [-0.8140, -4.5701,  2.3966,  2.8336, -0.1575,  5.3245, -0.4004,  1.8420,
         -3.9213, -2.3566],
        [ 1.6518,  0.1281,  2.1819, -0.2296,  4.1228, -0.2814,  1.4106, -0.8388,
         -2.4360, -3.0393],
        [-1.2521, -3.1468,  0.3812,  0.9961,  4.1565,  1.7851,  0.2695,  6.6411,
         -5.3557, -1.3407]])
tensor([6, 5, 4, 7])
Predicted:   frog   dog  deer horse
</code></pre>
<pre><code class="language-python">correct = 0
total = 0
with torch.no_grad(): # é¢„æµ‹çš„è¯ä¸éœ€è¦è®¡ç®—gradient
    for data in testloader:
        images, labels = data
        outputs = net(images)
        _, predicted = torch.max(outputs.data, 1) 
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

print('Accuracy of the network on the 10000 test images: %d %%' % (
    100 * correct / total))
</code></pre>
<pre><code>Accuracy of the network on the 10000 test images: 56 %
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[DewarpNet]]></title>
        <id>https://stedylan.github.io/post/README/</id>
        <link href="https://stedylan.github.io/post/README/">
        </link>
        <updated>2020-12-11T16:00:00.000Z</updated>
        <content type="html"><![CDATA[<h1 id="dewarpnet">DewarpNet</h1>
<p>This repository contains the codes for <a href="https://www3.cs.stonybrook.edu/~cvl/projects/dewarpnet/storage/paper.pdf"><strong>DewarpNet</strong></a> training.</p>
<h3 id="recent-updates">Recent Updates</h3>
<ul>
<li><strong>[May, 2020]</strong> Added evaluation images and an important note about Matlab SSIM.</li>
<li><strong>[Dec, 2020]</strong> Added OCR evaluation details.</li>
</ul>
<h3 id="training">Training</h3>
<ul>
<li>Prepare Data: <code>train.txt</code> &amp; <code>val.txt</code>. Contents should be like:</li>
</ul>
<pre><code>1/824_8-cp_Page_0503-7Ns0001
1/824_1-cp_Page_0504-2Cw0001
</code></pre>
<ul>
<li>Train Shape Network:<br>
<code>python trainwc.py --arch unetnc --data_path ./data/DewarpNet/doc3d/ --batch_size 50 --tboard</code></li>
<li>Train Texture Mapping Network:<br>
<code>python trainbm.py --arch dnetccnl --img_rows 128 --img_cols 128 --img_norm --n_epoch 250 --batch_size 50 --l_rate 0.0001 --tboard --data_path ./DewarpNet/doc3d</code></li>
</ul>
<h3 id="inference">Inference:</h3>
<ul>
<li>Run:<br>
<code>python infer.py --wc_model_path ./eval/models/unetnc_doc3d.pkl --bm_model_path ./eval/models/dnetccnl_doc3d.pkl --show</code></li>
</ul>
<h3 id="evaluation-image-metrics">Evaluation (Image Metrics):</h3>
<ul>
<li>
<p>We use the same evaluation code as <a href="https://www3.cs.stonybrook.edu/~cvl/docunet.html">DocUNet</a>.<br>
To reproduce the quantitative results reported in the paper use the images available <a href="https://drive.google.com/drive/folders/1aPfQHGrGxpuIbYLONydbSkGNygRX2z2P?usp=sharing">here</a>.</p>
</li>
<li>
<p><strong>[Important note about Matlab version]</strong> We noticed that Matlab 2020a uses a different SSIM implementation which gives a better MS-SSIM score (0.5623). Whereas we have used Matlab 2018b. Please compare the scores according to your Matlab version.</p>
</li>
</ul>
<h3 id="evaluation-ocr-metrics">Evaluation (OCR Metrics):</h3>
<ul>
<li>The 25 images used for OCR evaluation is <code>/eval/ocr_eval/ocr_files.txt</code></li>
<li>The corresponding ground-truth text is given in <code>/eval/ocr_eval/tess_gt.json</code></li>
<li>For the OCR errors reported in the paper we had used cv2.blur as pre-processing which gives higher error in all the cases. For convenience, we provide the updated numbers (without using blur) in the following table:</li>
</ul>
<table>
<thead>
<tr>
<th style="text-align:center">Method</th>
<th style="text-align:center">ED</th>
<th style="text-align:center">CER</th>
<th style="text-align:center">ED  (no blur)</th>
<th style="text-align:center">CER (no blur)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">DocUNet</td>
<td style="text-align:center">1975.86</td>
<td style="text-align:center">0.4656(0.263)</td>
<td style="text-align:center">1671.80</td>
<td style="text-align:center">0.403 (0.256)</td>
</tr>
<tr>
<td style="text-align:center">DocUNet on Doc3D</td>
<td style="text-align:center">1684.34</td>
<td style="text-align:center">0.3955 (0.272)</td>
<td style="text-align:center">1296.00</td>
<td style="text-align:center">0.294 (0.235)</td>
</tr>
<tr>
<td style="text-align:center">DewarpNet</td>
<td style="text-align:center">1288.60</td>
<td style="text-align:center">0.3136 (0.248)</td>
<td style="text-align:center">1007.28</td>
<td style="text-align:center">0.249 (0.236)</td>
</tr>
<tr>
<td style="text-align:center">DewarpNet (ref)</td>
<td style="text-align:center">1114.40</td>
<td style="text-align:center">0.2692 (0.234)</td>
<td style="text-align:center">812.48</td>
<td style="text-align:center">0.204 (0.228)</td>
</tr>
</tbody>
</table>
<ul>
<li>We had used the Tesseract (v4.1.0) default configuration for evaluation with PyTesseract (v0.2.6).</li>
</ul>
<h3 id="models">Models:</h3>
<ul>
<li>Pre-trained models are available <a href="https://drive.google.com/file/d/1hJKCb4eF1AJih_dhZOJSF5VR-ZtRNaap/view?usp=sharing">here</a>. These models are captured prior to  end-to-end training, thus won't give you the end-to-end results reported in Table 2 of the paper. Use the images provided above to get the exact numbers as Table 2.</li>
</ul>
<h3 id="dataset">Dataset:</h3>
<ul>
<li>The <em>doc3D dataset</em> can be downloaded using the scripts <a href="https://github.com/cvlab-stonybrook/doc3D-dataset">here</a>.</li>
</ul>
<h3 id="more-stuff">More Stuff:</h3>
<ul>
<li><a href="https://sagniklp.github.io/dewarpnet-demo/">Demo</a></li>
<li><a href="https://www3.cs.stonybrook.edu/~cvl/projects/dewarpnet/">Project Page</a></li>
<li><a href="https://github.com/sagniklp/doc3D-renderer">Doc3D Rendering Codes</a></li>
</ul>
<h3 id="citation">Citation:</h3>
<p>If you use the dataset or this code, please consider citing our work-</p>
<pre><code>@inproceedings{SagnikKeICCV2019, 
Author = {Sagnik Das*, Ke Ma*, Zhixin Shu, Dimitris Samaras, Roy Shilkrot}, 
Booktitle = {Proceedings of International Conference on Computer Vision}, 
Title = {DewarpNet: Single-Image Document Unwarping With Stacked 3D and 2D Regression Networks}, 
Year = {2019}}   
</code></pre>
<h4 id="acknowledgements">Acknowledgements:</h4>
<ul>
<li>These codes are heavily structured on <a href="https://github.com/meetshah1995/pytorch-semseg">pytorch-semseg</a>.</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hello Gridea]]></title>
        <id>https://stedylan.github.io/post/hello-gridea/</id>
        <link href="https://stedylan.github.io/post/hello-gridea/">
        </link>
        <updated>2018-12-11T16:00:00.000Z</updated>
        <summary type="html"><![CDATA[<p>ğŸ‘  æ¬¢è¿ä½¿ç”¨ <strong>Gridea</strong> ï¼<br>
âœï¸  <strong>Gridea</strong> ä¸€ä¸ªé™æ€åšå®¢å†™ä½œå®¢æˆ·ç«¯ã€‚ä½ å¯ä»¥ç”¨å®ƒæ¥è®°å½•ä½ çš„ç”Ÿæ´»ã€å¿ƒæƒ…ã€çŸ¥è¯†ã€ç¬”è®°ã€åˆ›æ„... ...</p>
]]></summary>
        <content type="html"><![CDATA[<p>ğŸ‘  æ¬¢è¿ä½¿ç”¨ <strong>Gridea</strong> ï¼<br>
âœï¸  <strong>Gridea</strong> ä¸€ä¸ªé™æ€åšå®¢å†™ä½œå®¢æˆ·ç«¯ã€‚ä½ å¯ä»¥ç”¨å®ƒæ¥è®°å½•ä½ çš„ç”Ÿæ´»ã€å¿ƒæƒ…ã€çŸ¥è¯†ã€ç¬”è®°ã€åˆ›æ„... ...</p>
<!-- more -->
<p><a href="https://github.com/getgridea/gridea">Github</a><br>
<a href="https://gridea.dev/">Gridea ä¸»é¡µ</a><br>
<a href="http://fehey.com/">ç¤ºä¾‹ç½‘ç«™</a></p>
<h2 id="ç‰¹æ€§">ç‰¹æ€§ğŸ‘‡</h2>
<p>ğŸ“  ä½ å¯ä»¥ä½¿ç”¨æœ€é…·çš„ <strong>Markdown</strong> è¯­æ³•ï¼Œè¿›è¡Œå¿«é€Ÿåˆ›ä½œ</p>
<p>ğŸŒ‰  ä½ å¯ä»¥ç»™æ–‡ç« é…ä¸Šç²¾ç¾çš„å°é¢å›¾å’Œåœ¨æ–‡ç« ä»»æ„ä½ç½®æ’å…¥å›¾ç‰‡</p>
<p>ğŸ·ï¸  ä½ å¯ä»¥å¯¹æ–‡ç« è¿›è¡Œæ ‡ç­¾åˆ†ç»„</p>
<p>ğŸ“‹  ä½ å¯ä»¥è‡ªå®šä¹‰èœå•ï¼Œç”šè‡³å¯ä»¥åˆ›å»ºå¤–éƒ¨é“¾æ¥èœå•</p>
<p>ğŸ’»  ä½ å¯ä»¥åœ¨ <strong>Windows</strong>ï¼Œ<strong>MacOS</strong> æˆ– <strong>Linux</strong> è®¾å¤‡ä¸Šä½¿ç”¨æ­¤å®¢æˆ·ç«¯</p>
<p>ğŸŒ  ä½ å¯ä»¥ä½¿ç”¨ <strong>ğ–¦ğ—‚ğ—ğ—ğ—ğ–» ğ–¯ğ–ºğ—€ğ–¾ğ—Œ</strong> æˆ– <strong>Coding Pages</strong> å‘ä¸–ç•Œå±•ç¤ºï¼Œæœªæ¥å°†æ”¯æŒæ›´å¤šå¹³å°</p>
<p>ğŸ’¬  ä½ å¯ä»¥è¿›è¡Œç®€å•çš„é…ç½®ï¼Œæ¥å…¥ <a href="https://github.com/gitalk/gitalk">Gitalk</a> æˆ– <a href="https://github.com/SukkaW/DisqusJS">DisqusJS</a> è¯„è®ºç³»ç»Ÿ</p>
<p>ğŸ‡¬ğŸ‡§  ä½ å¯ä»¥ä½¿ç”¨<strong>ä¸­æ–‡ç®€ä½“</strong>æˆ–<strong>è‹±è¯­</strong></p>
<p>ğŸŒ  ä½ å¯ä»¥ä»»æ„ä½¿ç”¨åº”ç”¨å†…é»˜è®¤ä¸»é¢˜æˆ–ä»»æ„ç¬¬ä¸‰æ–¹ä¸»é¢˜ï¼Œå¼ºå¤§çš„ä¸»é¢˜è‡ªå®šä¹‰èƒ½åŠ›</p>
<p>ğŸ–¥  ä½ å¯ä»¥è‡ªå®šä¹‰æºæ–‡ä»¶å¤¹ï¼Œåˆ©ç”¨ OneDriveã€ç™¾åº¦ç½‘ç›˜ã€iCloudã€Dropbox ç­‰è¿›è¡Œå¤šè®¾å¤‡åŒæ­¥</p>
<p>ğŸŒ± å½“ç„¶ <strong>Gridea</strong> è¿˜å¾ˆå¹´è½»ï¼Œæœ‰å¾ˆå¤šä¸è¶³ï¼Œä½†è¯·ç›¸ä¿¡ï¼Œå®ƒä¼šä¸åœå‘å‰ ğŸƒ</p>
<p>æœªæ¥ï¼Œå®ƒä¸€å®šä¼šæˆä¸ºä½ ç¦»ä¸å¼€çš„ä¼™ä¼´</p>
<p>å°½æƒ…å‘æŒ¥ä½ çš„æ‰åå§ï¼</p>
<p>ğŸ˜˜ Enjoy~</p>
]]></content>
    </entry>
</feed>